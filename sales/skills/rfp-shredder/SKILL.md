---
name: rfp-shredder
description: Analyzes PDF RFPs to determine Go/No-Go and draft answers. Extracts requirements, security questions, and compliance needs. Trigger with "analyze RFP", "read this PDF", "shred this RFP".
bundle: deal-execution
triggers:
  - "analyze RFP"
  - "read this RFP"
  - "shred this RFP"
tools:
  standalone: [filesystem]
  supercharged: []
version: 1.0.0
---

# Skill: RFP Shredder

Turn a 50-page PDF into a 1-page summary and a draft response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STANDALONE (always works)                                      â”‚
â”‚  âœ“ Analyze Fit Score (0-100) based on ICP and Capabilities      â”‚
â”‚  âœ“ Detect Red Flags (Unlimited Liability, Source Code Escrow)   â”‚
â”‚  âœ“ Extract Requirements Matrix (Must-Haves vs Nice-to-Haves)    â”‚
â”‚  âœ“ Draft standard answers for Security, GDPR, SLA               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SUPERCHARGED (connect ~~knowledge-base)                        â”‚
â”‚  + Search your previous proposals for "Best Answers"            â”‚
â”‚  + Auto-fill 60-80% of technical questions                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ›  Agent Instructions

### Phase 1: The Scan (Go/No-Go Analysis)

**Trigger**: "Analyze this RFP [upload PDF]."

**Actions**:

1.  **Read the PDF**: Scan for keywords.
2.  **Run Red Flag Check**: Compare against `references/red-flags.md`.
    - _Examples_: "Liquidated Damages", "On-premise only", "24/7 Phone Support".
3.  **Calculate Fit Score**: Use `references/scoring-matrix.md`.
    - _Criteria_: Budget, Timeline, Tech Stack match, Competitors.
4.  **Recommendation**: "GO" (Score > 70) or "NO-GO" (Score < 50).

**Output**: A "Decision Memo" with the Score and Top 3 Risks.

### Phase 2: The Shred (Extraction)

**Trigger**: "Summarize the requirements."

**Actions**:

1.  **Extract Key Data**:
    - **Deadlines**: Submission date, Q&A date.
    - **Format**: Word/PDF? Portal upload?
    - **Contacts**: Buyer name and role.
2.  **Build Requirements Matrix**:
    - Table columns: `Requirement | Compliance (Y/N/Partial) | Effort to Build`.

**Output**: A Markdown table of the "Must-Haves".

### Phase 3: The Draft (First Pass)

**Trigger**: "Draft answers for Section 4 (Security)."

**Actions**:

1.  **Load Context**: Read `data/2-Domaines/sales-profile.json` (Product Info).
2.  **Load Templates**: Read `references/response-templates.md`.
3.  **Draft Answers**:
    - If question is standard ("Data Privacy"), paste the GDPR snippet.
    - If question is specific ("Do you integrate with Legacy System X?"), draft a "Partial" answer emphasizing API capabilities.

## ðŸ“‚ System Integration

- **Language Logic**:
  - Check `sales-profile.json` or PDF language.
  - If French: Use "Appel d'Offres", "Cahier des Charges", "RGPD".
- **ICP Alignment**:
  - Load `data/2-Domaines/icp.json`.
  - If the RFP asks for features NOT in your ICP (e.g. Enterprise Customization for a SMB tool), flag as NO-GO.

## ðŸ“š References

- `references/red-flags.md`: List of terms that kill deals.
- `references/scoring-matrix.md`: The math behind the decision.
- `references/response-templates.md`: Standard answers for Security, SLA, Support.

---

## ðŸ”¬ Sentinel Integration (when installed)

```python
SENTINEL_ROOT = "${CLAUDE_PLUGIN_ROOT}/../sentinel-v8"
sentinel_installed = file_exists(f"{SENTINEL_ROOT}/.claude-plugin/plugin.json")
```

Skip this section entirely if `sentinel_installed` is False. RFP Shredder works identically without it.

### Trigger condition
Activate only when Phase 1 produces a **borderline score (50â€“79)**. 
- Score â‰¥ 80: Clear GO â€” Sentinel overhead not justified, proceed to Phase 2
- Score < 50: Clear NO-GO â€” Sentinel can't rescue a bad fit
- Score 50â€“79: Decision is genuinely uncertain â€” this is where bias causes the most damage

### What to run

**Step 1 â€” questioner (bid/no-bid bias check)**

Load bias IDs: `[7, 14, 18, 1]`
- ID 7: Sunk Cost ("we already spent 2 hours reading this")
- ID 14: Overconfidence ("we can make this work")
- ID 18: Status Quo ("we usually bid on RFPs like this")
- ID 1: Anchoring (score of 65 anchors team to "borderline GO")

Generate 3â€“4 questions targeting the real bid/no-bid factors:
- "Is there an incumbent? What evidence do you have that the buyer is genuinely open to switching?"
- "Who wrote this RFP â€” a real buyer or a procurement process? Have you spoken to anyone at this company?"
- "If you hadn't already read 30 pages of this RFP, would you bid on it based on this summary?"
- "What's your win rate on RFPs from buyers you've never met? Is that rate acceptable for the time investment here?"

**Step 2 â€” failure-finder (bid pre-mortem)**

For borderline GO decisions, invoke `failure-finder`:

> "You submitted the proposal. It's 90 days later. You lost. What happened?"

Generate 4 failure modes specific to RFP contexts:
- Hidden incumbent (RFP was written for someone else, you were never a real option)
- Relationship deficit (evaluation was won before the RFP was published, through conversations you weren't part of)
- Requirements mismatch (a must-have you marked Partial was actually disqualifying)
- Resource overcommit (team was stretched writing the response, quality suffered)

**Step 3 â€” calibration record**

For every GO decision (including clear GOs), record a calibration prediction:
> "Confidence you will win this RFP: [X%]"

Write to `../sentinel-v8/data/decision-ledger.json` via `calibration-coach`.
After 10+ RFPs: run `/sentinel-review` to see your actual win rate vs. stated confidence.

### Output format addition

For borderline scores, insert a **Bid Hygiene** section between the score and the recommendation:

```
## Bid Hygiene (Sentinel) â€” Score: [X]/100

Questions to answer before committing:
1. [questioner question â€” relationship status]
2. [questioner question â€” sunk cost check]
3. [questioner question â€” incumbent signal]

Pre-mortem top risk:
â†’ [highest likelihood failure mode] â€” Early warning: [signal to watch]

Decision: [GO / NO-GO]
Confidence recorded: [X%] â€” tracked for win-rate calibration.
```

### Standalone output (Sentinel not installed)
Standard Decision Memo with fit score, red flags, and GO / NO-GO recommendation. No change.

---

## ðŸ›¡ï¸ Sentinel Integration

```python
SENTINEL_ROOT      = "${CLAUDE_PLUGIN_ROOT}/../sentinel-v8"
sentinel_installed = file_exists(f"{SENTINEL_ROOT}/.claude-plugin/plugin.json")
```

**Trigger**: After Phase 1 score is calculated, if score is 50â€“80 (borderline GO) OR any time score > 80 and a red flag was detected.
**Standalone mode**: Decision memo stands as-is.

### If Sentinel installed â€” Bid/no-bid hygiene check

**Step A â€” questioner** (`../sentinel-v8/agents/questioner.md`)

Pass `bias_ids: [7, 14, 18, 29]` (Sunk Cost, Overconfidence, Status Quo, Groupthink).

Ask 3 questions targeting bid/no-bid traps:

1. **Sunk cost check**: "You've read the RFP. That investment of time creates pull toward GO. If you received this RFP today and hadn't read it yet â€” knowing only the score and red flags â€” would you decide to read it?"

2. **Incumbent check**: "Is there any signal in the RFP that it was written with a specific vendor in mind? Custom requirements, specific certifications, unusual scoring weights?" (RFPs written around an incumbent account for ~40% of lost bids.)

3. **Relationship check**: "Do you have a contact at the buying organization who can tell you whether this is a real open competition or a compliance exercise?" If no: flag RISK.

**Step B â€” failure-finder** (`../sentinel-v8/agents/failure-finder.md`)

Mode 1 (pre-mortem) for any GO decision. Frame:
> "It's 90 days after submission. You lost the bid. What happened?"

Generate 4 failure modes specific to RFP sales:
- `incumbent_advantage` â€” the winner had a relationship you didn't know about
- `scope_mismatch` â€” you bid on what they asked, not what they actually need
- `price_miscalibration` â€” your price was outside their unstated budget ceiling
- `team_thin` â€” the proposal was strong but your delivery team didn't match the evaluation criteria

For each: likelihood (HIGH/MEDIUM/LOW) + prevent_by action you can take before submission.

**Step C â€” calibration record**

For every GO decision, record:
```json
{
  "decision": "RFP bid: [client/project]",
  "prediction": "Will win this bid",
  "confidence": <fit_score / 100 as float>,
  "review_date": "<expected award date>",
  "plugin": "sales/rfp-shredder"
}
```

Over time, calibration-coach tracks your actual RFP win rate vs your confidence scores â€” revealing whether your fit-score model is calibrated.

**Output integration** â€” insert between Phase 1 Decision Memo and Phase 2 Extraction:

```
## Bid/No-Go Hygiene (Sentinel)

**Pre-bid questions to answer before proceeding:**
1. [Sunk cost question]
2. [Incumbent signal question]
3. [Relationship question]

**Pre-mortem failure modes:**
| Risk | Likelihood | Prevention |
|------|-----------|------------|
| [Mode 1] | HIGH | [Action] |
| [Mode 2] | MEDIUM | [Action] |

**Prediction recorded** â€” win confidence: [fit_score]% â€” review: [award date]

---
```
